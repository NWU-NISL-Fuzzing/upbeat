{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook can be divided into three parts: (1) a single case to demonstrate the entire Upbeat pipeline, (2) a small-scale test to show the capability of Upbeat, and (3) the evaluation presented in our paper.\n",
    "\n",
    "The complete experiment in our paper included more test cases and ran for a longer duration. However, this notebook aims to use the minimal working examples to explore the capabilities of Upbeat at an affordable cost. \n",
    "\n",
    "## Instructions for Running the Notebook\n",
    "\n",
    "1. Copy the Notebook: Make a copy of `main.ipynb`.\n",
    "2. Restart and Clear Output: Open the copied notebook and click `Restart` from the Kernel menu.\n",
    "3. Run the Notebook: Press the play button to execute each cell sequentially. **Ensure you wait for each cell to complete before proceeding to the next one.**\n",
    "\n",
    "## Link to Paper\n",
    "\n",
    "In the following sections, we provide the corresponding section numbers from our paper, where the relevant techniques are described, or the data is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Case Study\n",
    "\n",
    "In this section, you can observe how a test case is generated and tested by Upbeat. It will take approximately 1 minute to run all cells in this section.\n",
    "\n",
    "First, we need to import necessary tools and load code segment database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import os\n",
    "os.chdir(\"../src/Generate\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from basic_operation.dict_operation import get_rest_args\n",
    "from DBOperation.dboperation_sqlite import DataBaseHandle\n",
    "from combine_fragment import CodeFragmentGenerator, get_contained_cons\n",
    "from cons_generator.cwvp import generate_if_cons_exist\n",
    "from generate import Generate\n",
    "from class_for_info.fragment_info import CodeFragmentInfo\n",
    "\n",
    "generate = Generate(\"../config.json\")\n",
    "fragment = CodeFragmentGenerator(1, \"CodeFragment_CW\")\n",
    "frag_db = DataBaseHandle(\"../../data/query/corpus-v3.db\")\n",
    "frag_list = frag_db.selectAll(\"select * from CodeFragment_CW\")\n",
    "\n",
    "class colors:\n",
    "    \"\"\" Define colors for printing. \"\"\"\n",
    "    \n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Test Codes & Test Input Datas (Section 3.2 & Section 3.3)\n",
    "\n",
    "Then, we can select a code segment from corpus. `pre-condition` contains a set of variable necessary for the successful execution of the code segment, and `post-conditions` contains the available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a code segment from database.\n",
    "frag = random.choice(frag_list)\n",
    "# Get the pre-conditions of the code segment.\n",
    "available_variables = ast.literal_eval(frag[2])\n",
    "# Get the post-conditions of the code segment.\n",
    "needful_variables = ast.literal_eval(frag[3])\n",
    "# Print the information of the code segment.\n",
    "print(colors.BOLD+\"content of code segment:\\n\"+colors.RESET+frag[1]+\n",
    "      colors.BOLD+\"pre-conditions:\"+colors.RESET,needful_variables,\n",
    "      \"\\n\"+colors.BOLD+\"post-conditions:\"+colors.RESET,available_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a test code is synthesized via assembling type-directed code segments. \n",
    "\n",
    "If constraints are present in the synthesized test code, Upbeat generates both valid and invalid values; otherwise, Upbeat will generate a specified boundary value or random value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except variables that are related to constraints.\n",
    "bool_expr_list, func_ret_list, quanternion_list, needful_args, partial_reset_stmt = get_contained_cons(frag[1], needful_variables)\n",
    "needful_variables = get_rest_args(needful_variables, needful_args)\n",
    "# Start to generate the test code.\n",
    "this_fragment_info = CodeFragmentInfo(frag[1], available_variables, needful_variables, frag[4], frag[5], frag[6])\n",
    "combined_fragment, combined_import = fragment.generate_a_code_frag(this_fragment_info)\n",
    "if combined_fragment is None:\n",
    "    print(colors.RED+\"!!!Generation failed!\"+colors.RESET)\n",
    "else:\n",
    "    combined_fragment = combined_fragment.replace(\"\\n\\n\", \"\\n\").replace(\"//no cons\\n//no cons\\n\", \"//no cons\\n\")\n",
    "# For variables related to constraints, generate valid and invalid values.\n",
    "if len(bool_expr_list) > 0 or len(func_ret_list) > 0:\n",
    "    print(colors.BLUE+\"There exist constraints! Start to generate inputs.\"+colors.RESET)\n",
    "    valid_stmt, invalid_stmt = \\\n",
    "        generate_if_cons_exist(bool_expr_list, func_ret_list, quanternion_list, needful_args)\n",
    "else:\n",
    "    valid_stmt, invalid_stmt = \"\", \"\"\n",
    "# Check if generation failed.\n",
    "if valid_stmt is None:\n",
    "    print(colors.RED+\"!!!generation failed!\"+colors.RESET)\n",
    "# Print valid and invalid statements along with the combined fragment.\n",
    "else:\n",
    "    print(colors.BOLD+\"test code & valid inputs:\\n\"+colors.RESET+valid_stmt+combined_fragment+\\\n",
    "          colors.BOLD+\"\\ntest code & invalid inputs:\\n\"+colors.RESET+invalid_stmt+combined_fragment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Assemble Test Case (Section 3.4)\n",
    "\n",
    "Finally, Upbeat inserts all generated elements into the template. This includes the key components (variable declarations, a test code, and diagnostic statements) and other necessary content (import statements, reset statements, and callable declarations).\n",
    "\n",
    "If there are API constraints in the relevant code segments, Upbeat will generate two test cases (one for valid inputs and another for invalid inputs). Otherwise, Upbeat will generate a single test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_testcase, invalid_testcase = \"\", \"\"\n",
    "\n",
    "# Check if there are callable declarations needed for the generated test code.\n",
    "defined_callables = \"\"\n",
    "for item in fragment.self_defined_callables:\n",
    "    if isinstance(item, str):\n",
    "        if item not in defined_callables:\n",
    "            defined_callables += item\n",
    "    else:\n",
    "        if item.content not in defined_callables:\n",
    "            defined_callables += item.content\n",
    "\n",
    "# Process valid test case.\n",
    "valid_fragment = valid_stmt+combined_fragment+partial_reset_stmt\n",
    "valid_import = combined_import\n",
    "valid_testcase = generate.assemble_testcase(valid_fragment, valid_import, [defined_callables])\n",
    "print(colors.BOLD+\"valid testcase:\\n\"+colors.RESET+valid_testcase)\n",
    "\n",
    "# Process invalid test case.\n",
    "if invalid_stmt is None or invalid_stmt == valid_stmt:\n",
    "    print(colors.BOLD+\"\\ninvalid testcase:\\nSame as below.\"+colors.RESET)\n",
    "else:\n",
    "    invalid_fragment = invalid_stmt+combined_fragment+partial_reset_stmt\n",
    "    invalid_import = combined_import\n",
    "    invalid_testcase = generate.assemble_testcase(invalid_fragment, invalid_import, [defined_callables])\n",
    "    print(colors.BOLD+\"\\ninvalid testcase:\\n\"+colors.RESET+invalid_testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Execute Test Cases (Section 3.5)\n",
    "There are two test oracles in Upbeat: language-level testing via constraints and differential testing. \n",
    "\n",
    "In language-level testing, any results deviate from expected behaviors, crashed and timeouts will be served as anomalous. \n",
    "\n",
    "**Note:** While we have taken care to avoid syntax errors in the generated test cases, some may still fail during the dotnet build process. If you see `The build failed. Fix the build errors and run again.` displayed, please generate a new test case and rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Fuzzing\")\n",
    "\n",
    "from Fuzzing.lib.Harness import *\n",
    "from Fuzzing.lib.post_processor import *\n",
    "from Generate.basic_operation.file_operation import initParams\n",
    "\n",
    "# You can also change `valid_testcase`` into `invalid_testcase` if they are different.\n",
    "testcase_content = valid_testcase\n",
    "\n",
    "print(colors.BOLD+\"==language-level testing==\"+colors.RESET)\n",
    "print(\"Please wait a few seconds until you see the message 'nothing happended' or '!!!find anomaly'.\")\n",
    "susp_flag = False\n",
    "# Record the expected behavior from the test case.\n",
    "# These comments are added when generating variables containing constraints.\n",
    "if \"//wrong\" in testcase_content or \"//invalid\" in testcase_content:\n",
    "    flag = 0\n",
    "elif \"//correct\" in testcase_content or \"//valid\" in testcase_content:\n",
    "    flag = 1\n",
    "else:\n",
    "    flag = -1\n",
    "# Execute test case.\n",
    "output = execute(0, testcase_content, [\"dotnet\", \"run\"], False)\n",
    "print(colors.BOLD+\"output1:\\n\"+colors.RESET+output.stdout)\n",
    "# Detect anomalous: (1) if results deviate from expected behaviors; (2) crashed; or (3) timeout.\n",
    "if  ((flag == 1 and output.returnCode != 0) or \n",
    "    (flag == 0 and output.returnCode == 0) or \n",
    "    output.outputClass in [\"timout\", \"crash\"]):\n",
    "    print(colors.RED+\"!!!find anomaly\"+colors.RESET)\n",
    "    susp_flag = True\n",
    "else:\n",
    "    print(colors.GREEN+\"nothing happened\"+colors.RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In differential testing, Upbeat will detect any inconsistency, crash, or timeout. \n",
    "\n",
    "**Note:** The voting scheme has filtered some simple faulty behaviors. For example, ToffoliSimulator only supports parts of basic gates, it will throw a `NotImplementedException` exception if the test cases contain unsupported callables. Upbeat does not compare this exception with other results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential testing activates only if language-level testing detects no anomalies.\n",
    "if not susp_flag and output.returnCode not in [134, 137]:\n",
    "    outputs = [output]\n",
    "    command_list = [[\"dotnet\", \"run\", \"-s\", \"SparseSimulator\"],\n",
    "                    [\"dotnet\", \"run\", \"-s\", \"ToffoliSimulator\"]]\n",
    "    # Execute the test case on SparseSimulator and ToffoliSimulator.\n",
    "    for i, cmd in enumerate(command_list, start=2):\n",
    "        tmp_output = execute(0, output.testcaseContent, cmd, False)\n",
    "        outputs.append(tmp_output)\n",
    "        print(colors.BOLD+\"output\"+str(i)+\":\\n\"+colors.RESET+tmp_output.stdout)\n",
    "    # Perform the voting scheme.\n",
    "    vote(outputs, output.testcaseContent)\n",
    "else:\n",
    "    print(\"Already be anomalous in the language-level testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** You can repeatedly run the above cells to generate and test numerous test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Filter Anomalous (An extra module)\n",
    "\n",
    "To reduce costs during manual analysis, we integrate a straightforward module in Upbeat. Upbeat classifies anomalies into three categories:\n",
    "+ Bugs that have already been analyzed.\n",
    "+ Faulty cases that have already been analyzed.\n",
    "+ New anomalies that are awaiting verification.\n",
    "\n",
    "In the following cell, `filter_boundary()` is used to filter anomalies in language-level testing, and `filter_differential()` is used to filter anomalies in differential testing.\n",
    "\n",
    "**Note:** If differential testing is not performed, there are two options before runing the following cell: (1) Generate new test cases and run the test again. (2) Comment out the last line in the following cell (i.e., filter_differential(result_db)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "from Fuzzing.history_bug_filter import filter_boundary, filter_differential\n",
    "\n",
    "# If there exists a version of filtered results,\n",
    "# only read the results and display them.\n",
    "if os.path.exists(\"new_anomalies.txt\"):\n",
    "    print(\"There already exists a version of filtered results. \")\n",
    "    with open(\"bug.txt\", \"r\") as f1:\n",
    "        content1 = f1.read()\n",
    "    print(colors.BOLD+\"Bugs that have already been analyzed:\"+colors.RESET)\n",
    "    if content1 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content1)\n",
    "    with open(\"faulty.txt\", \"r\") as f2:\n",
    "        content2 = f2.read()\n",
    "    print(colors.BOLD+\"Faulty that have already been analyzed:\"+colors.RESET)\n",
    "    if content2 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content2)\n",
    "    with open(\"new_anomalies.txt\", \"r\") as f3:\n",
    "        content3 = f3.read()\n",
    "    print(colors.BOLD+\"New anomalies awaiting verification:\"+colors.RESET)\n",
    "    if content3 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content3)    \n",
    "# Otherwise, filter anomalies from language-level testing and differential testing.\n",
    "else:\n",
    "    print(\"Start to filter.\")\n",
    "    result_db = DataBaseHandle(\"../../data/result/UPBEAT.db\")\n",
    "    history_db = DataBaseHandle(\"../../data/query/history-bugs.db\")\n",
    "    filter_boundary(result_db, history_db)\n",
    "    filter_differential(result_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Small-Scale Testing\n",
    "\n",
    "To better demonstrate the capability of Upbeat, a small-scale test is performed in this section. It will take about 1 hour, please wait before moving to the next cell.\n",
    "\n",
    "**Tips:** To adjust the total number of test cases, you can modify the value of `fragment_num` in [config.json](src/config.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "os.chdir(\"/root/upbeat/src/Generate\")\n",
    "from main import main as generate_testcases\n",
    "\n",
    "# Generate test cases.\n",
    "# Approximate 2 seconds for 100 test cases.\n",
    "generate_testcases()\n",
    "\n",
    "# Apply language-level testing and differential testing. \n",
    "# Approximate 50 minuts for 100 test cases.\n",
    "os.chdir(\"../Fuzzing\")\n",
    "from hybrid_testing import main as testing\n",
    "from history_bug_filter import main as filtering\n",
    "\n",
    "testing()\n",
    "\n",
    "# Filter anomalies.\n",
    "# Approximate 2 seconds for 100 test cases.\n",
    "os.chdir(\"../\")\n",
    "filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Experimental Results\n",
    "\n",
    "In our paper, we design four RQs to evaluate Upbeat:\n",
    "* RQ1: How effectively Upbeat is on detecting boundary bugs in Q# libraries?\n",
    "* RQ2: How does Upbeat compare with prior methods and baselines on bug detection?\n",
    "* RQ3: How do individual components of Upbeat contribute to its overall performance?\n",
    "* RQ4: How effective is Upbeat in extracting constraints from Q# libraries and API documents?\n",
    "\n",
    "Please run the following cells to view our experiment results.\n",
    "\n",
    "### 3.1 Results for RQ1 (Section 5.1)\n",
    "\n",
    "During our experiment period, Upbeat has uncovered 16 implementation bugs and 4 API document errors. To review all the bugs detected by Upbeat during this period, please run the following cell. The results corresponds to Table 1 in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/root/upbeat/jupyter\")\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_tables_from_md(md_file):\n",
    "    \"\"\" \n",
    "    Extracts Markdown tables from a given Markdown file.\n",
    "    \n",
    "    Args:\n",
    "        md_file (str): The path to the Markdown file.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of Markdown tables found in the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the Markdown file specified by md_file in read mode\n",
    "    with open(md_file, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "    # Regular expression pattern to match Markdown tables\n",
    "    table_pattern = r'\\|.*\\|[\\s\\S]*?\\n(?=\\n|\\Z)'\n",
    "    # Find all occurrences of tables in md_content using the pattern\n",
    "    tables = re.findall(table_pattern, md_content)\n",
    "    return tables\n",
    "\n",
    "md_file = '../data/experiment/BugList.md'\n",
    "# Extract tables from the Markdown file\n",
    "tables = extract_tables_from_md(md_file)\n",
    "# Print each table found\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Results for RQ2 (Section 5.2)\n",
    "\n",
    "To answer RQ2, we compare Upbeat to the eight baselines, including QSharp-Fuzz, Quito, QSharpCheck, Muskit, QDiff, MorphQ, Upbeat-M and Upbeat-r. \n",
    "\n",
    "We use two metrics: code coverage and bug-exposing capability. The code coverage measures the code coverage for the Q# library APIs. The bug-exposing capability represents the number of unique anomalies.\n",
    "\n",
    "Upbeat outperforms the competing baselines by providing better code coverage and identifying more potential bugs with the same test time. Execute the following two cells to observe the coverage and anomaly results. The results corresponds to Figure 11 and Figure 12 from our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline, interp1d\n",
    "\n",
    "from Fuzzing.calculate_code_coverage import calculate_coverage\n",
    "\n",
    "color_list = ['#9EB3C2', '#AFCAD0', '#C0E0DE', '#8BC3D9', '#6EACC7', '#468FAF', '#297596', '#014F86', '#013A63']\n",
    "tool_list = ['qsharpfuzz', 'quito', 'qsharpcheck', 'upbeat-m', 'muskit', 'qdiff', 'morphq', 'upbeat-r', 'upbeat']\n",
    "\n",
    "def draw_one_line(y, label, color):\n",
    "    \"\"\" \n",
    "    Draw a smooth curve line with a given label and color.\n",
    "\n",
    "    Args:\n",
    "        y (list): A list of y-values.\n",
    "        label (str): The label for the line chart.\n",
    "        color (str): The color for the line chart.\n",
    "    \"\"\"\n",
    "\n",
    "    x = range(0, 25)\n",
    "    x_list = np.linspace(0, 24, 50)\n",
    "    f = interp1d(x, y, kind='linear')\n",
    "    y_list = f(x_list)\n",
    "    plt.plot(x_list, y_list, label=label, color=color)\n",
    "\n",
    "input_folder = \"../data/experiment/cov-result-origin/\"\n",
    "output_folder = \"../data/experiment/cov-result-calculated/\"\n",
    "line_cov_list, block_cov_list = [], []\n",
    "\n",
    "# Get the coverage results of all tools\n",
    "for tool, color in zip(tool_list, color_list):\n",
    "    line_cov, block_cov = [0.0], [0.0]\n",
    "    output_file = tool+\".txt\"\n",
    "    with open(output_folder+output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        block_cov.append(float(line.split(\" \")[1]))\n",
    "        line_cov.append(float(line.split(\" \")[2]))\n",
    "    line_cov_list.append(line_cov)\n",
    "    block_cov_list.append(block_cov)\n",
    "\n",
    "# Draw the plot picture for the line coverage\n",
    "plt.figure(figsize=(6, 4))\n",
    "for line_cov, tool, color in zip(line_cov_list, tool_list, color_list):\n",
    "    draw_one_line(line_cov, tool, color)\n",
    "plt.legend(fontsize='small')\n",
    "plt.xticks(np.arange(0, 25, 1))\n",
    "plt.yticks(np.arange(0, 60, 5))\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "\n",
    "# Draw the plot picture for the block coverage\n",
    "plt.figure(figsize=(6, 4))\n",
    "for block_cov, tool, color in zip(block_cov_list, tool_list, color_list):\n",
    "    draw_one_line(block_cov, tool, color)\n",
    "plt.legend(fontsize='small')\n",
    "plt.xticks(np.arange(0, 25, 1))\n",
    "plt.yticks(np.arange(0, 45, 5))\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Get the anomaly results from lang_dir.\n",
    "lang_dir = \"../data/experiment/anomalies-lang/\"\n",
    "regex = r\"Can be detected by (.*)\\.\"\n",
    "lang_results, diff_results = {}, {}\n",
    "lang_dir = \"../data/experiment/anomalies-lang/\"\n",
    "for f in os.listdir(lang_dir):\n",
    "    with open(lang_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in lang_results:\n",
    "        lang_results[tool] += 1\n",
    "    else:\n",
    "        lang_results[tool] = 1\n",
    "\n",
    "# Print the table. \n",
    "print(tabulate(lang_results.items(), headers=[\"Tool\", \"#Anomalies via language-level test\"]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the anomaly results from diff_dir.\n",
    "diff_dir = \"../data/experiment/anomalies-diff/\"\n",
    "for f in os.listdir(diff_dir):\n",
    "    with open(diff_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in diff_results:\n",
    "        diff_results[tool] += 1\n",
    "    else:\n",
    "        diff_results[tool] = 1\n",
    "\n",
    "# Print the table.\n",
    "print(tabulate(diff_results.items(), headers=[\"Tool\", \"#Anomalies via differential testing\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe these anomalous behaviors in detail, you can execute all test cases by running the following cell. This will take about 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/root/upbeat/jupyter\")\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "def run_one_testcase(cmd, dst_folder, testcase):\n",
    "    \"\"\"\n",
    "    Run one test case.\n",
    "\n",
    "    Parameters:\n",
    "        cmd (list): The list of command.\n",
    "        dst_folder (str): The destination folder where the test case is executed.\n",
    "        testcase (str): The name of the test case.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the bin and obj folders.\n",
    "    for root, dirs, files in os.walk(dst_folder):\n",
    "        for dirName in dirs:\n",
    "            if dirName == \"bin\" or dirName == \"obj\":\n",
    "                shutil.rmtree(os.path.join(dst_folder, dirName), ignore_errors=True)\n",
    "    # Run the test case.\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=dst_folder)\n",
    "    # If the output contains \"InvalidOperationException\", skip the test case.\n",
    "    if \"InvalidOperationException\" in result.stdout:\n",
    "        return\n",
    "    # Print the output.\n",
    "    print(colors.BOLD+\"Output of \"+\" \".join(cmd)+f\" for {testcase}:\"+colors.RESET)\n",
    "    print(result.stdout)\n",
    "    # print(result.stderr)\n",
    "\n",
    "def run_all_testcases(src_folder, dst_folder, cmd_list):\n",
    "    \"\"\"\n",
    "    Run all test cases.\n",
    "\n",
    "    Parameters:\n",
    "        src_folder (str): The source folder where the test cases are located.\n",
    "        dst_folder (str): The destination folder where the test cases are executed.\n",
    "        cmd_list (list): The list of command.\n",
    "    \"\"\"\n",
    "\n",
    "    testcases = os.listdir(src_folder)\n",
    "    testcases.sort(key = lambda x: int(x[7:x.index(\".\")]))\n",
    "    for testcase in testcases:\n",
    "        # Move the test case to the destination folder.\n",
    "        src_file = os.path.join(src_folder, testcase)\n",
    "        dst_file = os.path.join(dst_folder, \"Program.qs\")\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        # Run the test case by QuantumSimulator, SparseSimulator, and ToffoliSimulator.\n",
    "        for cmd in cmd_list:\n",
    "            run_one_testcase(cmd, dst_folder, testcase)\n",
    "\n",
    "proj_path = \"../data/experiment/temp-q-project\"\n",
    "\n",
    "# Run test cases from anomalies-lang.\n",
    "lang_path = \"../data/experiment/anomalies-lang/\"\n",
    "print(\"==Start to run anomalies via language-level testing==\")\n",
    "run_all_testcases(lang_path, proj_path, [['dotnet', 'run']])\n",
    "\n",
    "# Run test cases from anomalies-diff.\n",
    "diff_path = \"../data/experiment/anomalies-diff/\"\n",
    "print(\"==Start to run anomalies via differential testing==\")\n",
    "run_all_testcases(diff_path, proj_path, \n",
    "                  [['dotnet', 'run'], \n",
    "                  ['dotnet', 'run', '-s', 'ToffoliSimulator'], \n",
    "                  ['dotnet', 'run', '-s', 'SparseSimulator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Results for RQ3 (Section 5.3)\n",
    "\n",
    "In our ablation study, we evaluate two variants of Upbeat: Upbeat-A and Upbeat-B. Upbeat-A removes the inputs generator and keeps other parts of Upbeat unchanged. Upbeat-B removes the code segment assembler and keeps other components.\n",
    "\n",
    "Each component of Upbeat significantly enhances the framework's ability to expose bugs. Execute the following cells to view the bugs identified by Upbeat-A, Upbeat-B, and the complete Upbeat framework. The results correspond to Figure 13 in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ablation study results.\n",
    "abl_results = {}\n",
    "abl_dir = \"../data/experiment/ablation-study/\"\n",
    "for f in os.listdir(abl_dir):\n",
    "    with open(abl_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in abl_results:\n",
    "        abl_results[tool] += 1\n",
    "    else:\n",
    "        abl_results[tool] = 1\n",
    "\n",
    "# Print the table.\n",
    "print(tabulate(abl_results.items(), headers=[\"Tool\", \"#Bugs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Results for RQ4 (Section 5.4)\n",
    "\n",
    "We use two metrics, Recall and Precision, to evaluate the completeness and correctness of Upbeat-extracted constraints. \n",
    "\n",
    "Recall computes the proportion of Upbeat-identified constraints to the total number of constraints. Precision computes the ratio of accurately extracted constraint samples to the total identified constraints.\n",
    "\n",
    "Upbeat is capable of extracting the majority of constraints from both source code and API documents with high accuracy. Execute the subsequent cell to examine the detailed analysis results, corresponding to Table 2 in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def get_rate(num1: int, num2: int):\n",
    "    \"\"\"\n",
    "    Get the ratio of num1 to num2.\n",
    "    \n",
    "    Parameters:\n",
    "        num1 (int): The numerator.\n",
    "        num2 (int): The denominator.\n",
    "    \n",
    "    Returns:\n",
    "        float: The ratio of num1 to num2.\n",
    "    \"\"\"\n",
    "\n",
    "    if num2 == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return num1 / num2\n",
    "\n",
    "def convert_to_percent(n):\n",
    "    \"\"\"\n",
    "    Convert the ratio to percentage.\n",
    "    \n",
    "    Parameters:\n",
    "        n (float): The ratio.\n",
    "    \n",
    "    Returns:\n",
    "        str: The percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    n = round(n, 2)\n",
    "    return \"%.0f%%\" % (n * 100)\n",
    "\n",
    "def calculate(d: dict):\n",
    "    \"\"\"\n",
    "    Calculate recall and precision of constraints.\n",
    "\n",
    "    Parameters:\n",
    "       d (dict): The dictionary of analysis results.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The values of recall and precision.\n",
    "    \"\"\"\n",
    "\n",
    "    classical_id, classical_ex, quantum_id, quantum_ex = 0.0, 0.0, 0.0, 0.0\n",
    "    classical_id_total, classical_ex_total, quantum_id_total, quantum_ex_total = 0, 0, 0, 0\n",
    "    for namespace, properties in d.items():\n",
    "        classical_id += get_rate(properties[\"classical-identified\"], properties[\"classical-id-total\"])\n",
    "        classical_ex += get_rate(properties[\"classical-extracted\"], properties[\"classical-ex-total\"])\n",
    "        quantum_id += get_rate(properties[\"quantum-identified\"], properties[\"quantum-id-total\"])        \n",
    "        quantum_ex += get_rate(properties[\"quantum-extracted\"], properties[\"quantum-ex-total\"])\n",
    "        if properties[\"classical-id-total\"] != 0:\n",
    "            classical_id_total += 1\n",
    "        if properties[\"classical-ex-total\"] != 0:\n",
    "            classical_ex_total += 1\n",
    "        if properties[\"quantum-id-total\"] != 0:\n",
    "            quantum_id_total += 1\n",
    "        if properties[\"quantum-ex-total\"] != 0:\n",
    "            quantum_ex_total += 1\n",
    "    return convert_to_percent(classical_id / classical_id_total), convert_to_percent(classical_ex / classical_ex_total), \\\n",
    "           convert_to_percent(quantum_id / quantum_id_total), convert_to_percent(quantum_ex / quantum_ex_total)\n",
    "\n",
    "# Calculate the analysis results of constraints from source code. \n",
    "with open(\"../data/experiment/constraint-extraction/source-code.json\") as f1:\n",
    "    code_dict = json.load(f1)\n",
    "code_result = calculate(code_dict)\n",
    "tab = [(\"Source Code\", \"classical\", code_result[0], code_result[1]), (\"\", \"quantum\", code_result[2], code_result[3])]\n",
    "\n",
    "# Calculate the analysis results of constraints from API documents.\n",
    "with open(\"../data/experiment/constraint-extraction/api-document.json\") as f2:\n",
    "    doc_dict = json.load(f2)\n",
    "doc_result = calculate(doc_dict)\n",
    "tab.append((\"API Document\", \"classical\", doc_result[0], doc_result[1]))\n",
    "tab.append((\"\", \"quantum\", doc_result[2], doc_result[3]))\n",
    "\n",
    "# Print the tables.\n",
    "print(tabulate(tab, headers=[\"Source\", \"Type\", \"Recall\", \"Precision\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
