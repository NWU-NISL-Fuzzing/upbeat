{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This Jupyter Notebook can be divided into three parts: (1) a single case to demonstrate the entire Upbeat pipeline, (2) a small-scale test to show the capability of Upbeat, and (3) the evaluation presented in our paper.\n",
    "\n",
    "The complete experiment in our paper included more test cases and ran for a longer duration. However, this notebook focuses on minimal examples to explore the capabilities of Upbeat at an affordable cost.\n",
    "\n",
    "### Instructions for Running the Notebook\n",
    "\n",
    "1. Copy the Notebook: Please make a copy of main.ipynb.\n",
    "2. Restart and Clear Output: Open the copied notebook and select \"Restart & Clear Output\" from the Kernel menu.\n",
    "3. Run the Notebook: Press the play button to execute each cell sequentially. Ensure you wait for each cell to complete before proceeding to the next one.\n",
    "\n",
    "**Note:** It is crucial to wait for each step to finish. Moving to the next cell prematurely may cause errors due to incomplete data processing.\n",
    "\n",
    "### Link to Paper\n",
    "\n",
    "In the following sections, we provide the corresponding section numbers from our paper, where the relevant techniques are described, or the data is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Case Study\n",
    "\n",
    "First, we need to import necessary tools and load code segment database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import os\n",
    "os.chdir(\"../src/Generate\")\n",
    "\n",
    "from basic_operation.dict_operation import get_rest_args\n",
    "from DBOperation.dboperation_sqlite import DataBaseHandle\n",
    "from combine_fragment import CodeFragmentGenerator, get_contained_cons\n",
    "from cons_generator.cwvp import generate_if_cons_exist\n",
    "from generate import Generate\n",
    "from class_for_info.fragment_info import CodeFragmentInfo\n",
    "\n",
    "generate = Generate(\"../config.json\")\n",
    "fragment = CodeFragmentGenerator(1, \"CodeFragment_CW\")\n",
    "frag_db = DataBaseHandle(\"../../data/query/corpus-v3.db\")\n",
    "frag_list = frag_db.selectAll(\"select * from CodeFragment_CW\")\n",
    "\n",
    "class colors:\n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generate Test Codes & Test Input Datas (Section 3.2 & Section 3.3)\n",
    "\n",
    "Then, we can select a code segment from corpus. `pre-condition` contains a set of variable necessary for the successful execution of the code segment, and `post-conditions` contains the available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag = random.choice(frag_list)\n",
    "available_variables = ast.literal_eval(frag[2])\n",
    "needful_variables = ast.literal_eval(frag[3])\n",
    "print(colors.BOLD+\"content of code segment:\\n\"+colors.RESET+frag[1]+\n",
    "      colors.BOLD+\"pre-conditions:\"+colors.RESET,needful_variables,\n",
    "      \"\\n\"+colors.BOLD+\"post-conditions:\"+colors.RESET,available_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a test code is synthesized via assembling type-directed code segments. \n",
    "\n",
    "If constraints are present in the synthesized test code, Upbeat generates both valid and invalid values; otherwise, Upbeat will generate a specified boundary value or random value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except variables that are related to constraints\n",
    "bool_expr_list, func_ret_list, quanternion_list, needful_args, partial_reset_stmt = get_contained_cons(frag[1], needful_variables)\n",
    "needful_variables = get_rest_args(needful_variables, needful_args)\n",
    "# Start to assemble\n",
    "this_fragment_info = CodeFragmentInfo(frag[1], available_variables, needful_variables, frag[4], frag[5], frag[6])\n",
    "combined_fragment, combined_import = fragment.generate_a_code_frag(this_fragment_info)\n",
    "if combined_fragment is None:\n",
    "    print(colors.RED+\"!!!Generation failed!\"+colors.RESET)\n",
    "else:\n",
    "    combined_fragment = combined_fragment.replace(\"\\n\\n\", \"\\n\").replace(\"//no cons\\n//no cons\\n\", \"//no cons\\n\")\n",
    "print(colors.BOLD+\"==test code==\\n\"+colors.RESET+combined_fragment)\n",
    "if len(bool_expr_list) > 0 or len(func_ret_list) > 0:\n",
    "    print(colors.BLUE+\"There exist constraints! Start to generate inputs.\"+colors.RESET)\n",
    "    valid_stmt, invalid_stmt = \\\n",
    "        generate_if_cons_exist(bool_expr_list, func_ret_list, quanternion_list, needful_args)\n",
    "    # print(\"==valid==\\n\"+valid_stmt+\"\\n==invalid==\\n\"+invalid_stmt)\n",
    "else:\n",
    "    valid_stmt, invalid_stmt = \"//no cons\\n\", \"//no cons\\n\"\n",
    "# If generation failed\n",
    "if valid_stmt is None:\n",
    "    print(colors.RED+\"!!!Generation failed!\"+colors.RESET)\n",
    "else:\n",
    "    print(colors.BOLD+\"valid_stmt:\\n\"+colors.RESET+valid_stmt+\\\n",
    "          colors.BOLD+\"invalid_stmt:\\n\"+colors.RESET+invalid_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Assemble Test Case (Section 3.4)\n",
    "\n",
    "Finally, Upbeat inserts all generated elements into the template. This includes the key components (variable declarations, a test code, and diagnostic statements) and other necessary content (import statements, reset statements, and callable declarations).\n",
    "\n",
    "If there are API constraints in the relevant code segments, Upbeat will generate two test cases (one for valid inputs and another for invalid inputs). Otherwise, Upbeat will generate a single test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_testcase, invalid_testcase = \"\", \"\"\n",
    "# Get self_defined_callables\n",
    "defined_callables = \"\"\n",
    "for item in fragment.self_defined_callables:\n",
    "    if isinstance(item, str):\n",
    "        if item not in defined_callables:\n",
    "            defined_callables += item\n",
    "    else:\n",
    "        if item.content not in defined_callables:\n",
    "            defined_callables += item.content\n",
    "# Process valid test case\n",
    "valid_fragment = valid_stmt+combined_fragment+partial_reset_stmt\n",
    "valid_import = combined_import\n",
    "valid_testcase = generate.assemble_testcase(valid_fragment, valid_import, [defined_callables])\n",
    "print(colors.BOLD+\"==valid_testcase==\\n\"+colors.RESET+valid_testcase)\n",
    "# Process invalid test case\n",
    "if invalid_stmt is None or invalid_stmt == valid_stmt:\n",
    "    print(colors.BOLD+\"==invalid_testcase==\\nSame as below.\"+colors.RESET)\n",
    "else:\n",
    "    invalid_fragment = invalid_stmt+combined_fragment+partial_reset_stmt\n",
    "    invalid_import = combined_import\n",
    "    invalid_testcase = generate.assemble_testcase(invalid_fragment, invalid_import, [defined_callables])\n",
    "    print(colors.BOLD+\"==invalid_testcase==\\n\"+colors.RESET+invalid_testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Execute Test Cases (Section 3.5)\n",
    "There are two test oracles in Upbeat: language-level testing via constraints and differential testing. \n",
    "\n",
    "In language-level testing, any results deviate from expected behaviors, crashed and timeouts will be served as anomalous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Fuzzing\")\n",
    "\n",
    "from Fuzzing.lib.Harness import *\n",
    "from Fuzzing.lib.post_processor import *\n",
    "from Generate.basic_operation.file_operation import initParams\n",
    "\n",
    "# You can also change `valid_testcase`` into `invalid_testcase` if they are different\n",
    "testcase_content = valid_testcase\n",
    "\n",
    "print(colors.BOLD+\"==language-level testing==\"+colors.RESET)\n",
    "print(\"Please wait a few seconds until you see the message 'nothing happended' or '!!!find wrong cons'.\")\n",
    "susp_flag = False\n",
    "# Record expected behavior\n",
    "if \"//wrong\" in testcase_content or \"//invalid\" in testcase_content:\n",
    "    flag = 0\n",
    "elif \"//correct\" in testcase_content or \"//valid\" in testcase_content:\n",
    "    flag = 1\n",
    "else:\n",
    "    flag = -1\n",
    "# Execute test case\n",
    "output = execute(0, testcase_content, [\"dotnet\", \"run\"], False)\n",
    "print(colors.BOLD+\"output1:\\n\"+colors.RESET+output.stdout)\n",
    "# Detect anomalous\n",
    "if  ((flag == 1 and output.returnCode != 0) or \n",
    "    (flag == 0 and output.returnCode == 0) or \n",
    "    output.outputClass in [\"timout\", \"crash\"]):\n",
    "    print(colors.RED+\"!!!find wrong cons\"+colors.RESET)\n",
    "    susp_flag = True\n",
    "else:\n",
    "    print(\"\\033[92mnothing happened\"+colors.RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In differential testing, Upbeat will detect any inconsistency, crash, or timeout. \n",
    "\n",
    "**Note:** The voting scheme has filtered some simple faulty behaviors. For example, ToffoliSimulator only supports parts of basic gates, it will throw a `NotImplementedException` exception if the test cases contain unsupported callables. Upbeat does not compare this exception with other results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differential testing activates only if language-level testing detects no anomalies\n",
    "if not susp_flag and output.returnCode not in [134, 137]:\n",
    "    outputs = [output]\n",
    "    command_list = [[\"dotnet\", \"run\", \"-s\", \"SparseSimulator\"],\n",
    "                    [\"dotnet\", \"run\", \"-s\", \"ToffoliSimulator\"]]\n",
    "    # execute on SparseSimulator and ToffoliSimulator\n",
    "    for i, cmd in enumerate(command_list, start=2):\n",
    "        tmp_output = execute(0, output.testcaseContent, cmd, False)\n",
    "        outputs.append(tmp_output)\n",
    "        print(colors.BOLD+\"output\"+str(i)+\":\\n\"+colors.RESET+tmp_output.stdout)\n",
    "    # voting scheme\n",
    "    vote(outputs, output.testcaseContent)\n",
    "else:\n",
    "    print(\"Already be anomalous in the language-level testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Anomalous (An extra module)\n",
    "\n",
    "To reduce costs during manual analysis, we integrate a straightforward module in Upbeat. Upbeat classifies anomalies into three categories:\n",
    "+ Bugs that have already been analyzed.\n",
    "+ Faulty cases that have already been analyzed.\n",
    "+ New anomalies that are awaiting verification.\n",
    "\n",
    "In the following cell, `filter_boundary()` is used to filter anomalies in language-level testing, and `filter_differential()` is used to filter anomalies in differential testing.\n",
    "\n",
    "**Note:** If differential testing is not performed, there are two options: (1) Generate new test cases and run the test again. (2) Comment out the last line in the following cell (i.e., filter_differential(result_db)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "from Fuzzing.history_bug_filter import filter_boundary, filter_differential\n",
    "\n",
    "if os.path.exists(\"new_anomalies.txt\"):\n",
    "    print(\"There already exists a version of filtered results. \")\n",
    "    with open(\"bug.txt\", \"r\") as f1:\n",
    "        content1 = f1.read()\n",
    "    print(colors.BOLD+\"Bugs that have already been analyzed:\"+colors.RESET)\n",
    "    if content1 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content1)\n",
    "    with open(\"faulty.txt\", \"r\") as f2:\n",
    "        content2 = f2.read()\n",
    "    print(colors.BOLD+\"Faulty that have already been analyzed:\"+colors.RESET)\n",
    "    if content2 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content2)\n",
    "    with open(\"new_anomalies.txt\", \"r\") as f3:\n",
    "        content3 = f3.read()\n",
    "    print(colors.BOLD+\"New anomalies awaiting verification:\"+colors.RESET)\n",
    "    if content3 == \"\":\n",
    "        print(\"Nothing\")\n",
    "    else:\n",
    "        print(content3)    \n",
    "else:\n",
    "    print(\"Start to filter.\")\n",
    "    result_db = DataBaseHandle(\"../../data/result/UPBEAT.db\")\n",
    "    history_db = DataBaseHandle(\"../../data/query/history-bugs.db\")\n",
    "    filter_boundary(result_db, history_db)\n",
    "    filter_differential(result_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Small-Scale Testing\n",
    "\n",
    "To better demonstrate the capability of Upbeat, a small-scale test is performed here. It will take about 1 hour, please wait before moving to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Generate\")\n",
    "from main import main as generate_testcases\n",
    "\n",
    "# Approximate 2 seconds for 100 test cases.\n",
    "generate_testcases()\n",
    "\n",
    "# Approximate 50 minuts for 100 test cases.\n",
    "os.chdir(\"../Fuzzing\")\n",
    "from hybrid_testing import main as testing\n",
    "from history_bug_filter import main as filtering\n",
    "\n",
    "testing()\n",
    "\n",
    "os.chdir(\"../\")\n",
    "filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Experimental Results\n",
    "\n",
    "In our paper, we design four RQs to evaluate Upbeat:\n",
    "* RQ1: How effectively Upbeat is on detecting boundary bugs in Q# libraries?\n",
    "* RQ2: How does Upbeat compare with prior methods and baselines on bug detection?\n",
    "* RQ3: How do individual components of Upbeat contribute to its overall performance?\n",
    "* RQ4: How effective is Upbeat in extracting constraints from Q# libraries and API documents?\n",
    "\n",
    "Please run the following cells to view our experiment results.\n",
    "\n",
    "#### 3.1 Results for RQ1 (Section 5.2)\n",
    "\n",
    "During our experiment period, Upbeat has uncovered 16 implementation bugs and 4 API document errors. To review all the bugs detected by Upbeat during this period, please run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/root/upbeat/jupyter\")\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_tables_from_md(md_file):\n",
    "    with open(md_file, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "\n",
    "    table_pattern = r'\\|.*\\|[\\s\\S]*?\\n(?=\\n|\\Z)'\n",
    "    tables = re.findall(table_pattern, md_content)\n",
    "\n",
    "    return tables\n",
    "\n",
    "def main():\n",
    "    md_file = '../data/experiment/BugList.md'\n",
    "    tables = extract_tables_from_md(md_file)\n",
    "\n",
    "    for table in tables:\n",
    "        print(table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Results for RQ2 (Section 5.2)\n",
    "\n",
    "To answer RQ2, we compare Upbeat to the eight baselines, including QSharp-Fuzz, Quito, QSharpCheck, Muskit, QDiff, MorphQ, Upbeat-M and Upbeat-r. \n",
    "\n",
    "We use two metrics: code coverage and bug-exposing capability. The code coverage measures the code coverage for the Q# library APIs. The bug-exposing capability represents the number of unique anomalies.\n",
    "\n",
    "Upbeat outperforms the competing baselines by providing better code coverage and identifying more potential bugs with the same test time. Execute the following two cells to observe the coverage and anomaly results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline, interp1d\n",
    "\n",
    "from Fuzzing.calculate_code_coverage import calculate_coverage\n",
    "\n",
    "color_list = ['#9EB3C2', '#AFCAD0', '#C0E0DE', '#8BC3D9', '#6EACC7', '#468FAF', '#297596', '#014F86', '#013A63']\n",
    "tool_list = ['qsharpfuzz', 'quito', 'qsharpcheck', 'upbeat-m', 'muskit', 'qdiff', 'morphq', 'upbeat-r', 'upbeat']\n",
    "# marker_list = [',', 'o', '^', 'v', 'D', '<', '>', 'p', '*']\n",
    "\n",
    "def draw_one_line(y, label, color):\n",
    "    x = range(0, 25)\n",
    "    x_list = np.linspace(0, 24, 50)\n",
    "    f = interp1d(x, y, kind='linear')\n",
    "    y_list = f(x_list)\n",
    "    plt.plot(x_list, y_list, label=label, color=color)\n",
    "\n",
    "input_folder = \"../data/experiment/cov-result-origin/\"\n",
    "output_folder = \"../data/experiment/cov-result-calculated/\"\n",
    "# for input_file in os.listdir(input_folder):\n",
    "#     print(\"processing \"+input_file)\n",
    "#     calculate_coverage(input_folder+input_file, output_folder+input_file)\n",
    "line_cov_list, block_cov_list = [], []\n",
    "for tool, color in zip(tool_list, color_list):\n",
    "    line_cov, block_cov = [0.0], [0.0]\n",
    "    output_file = tool+\".txt\"\n",
    "    # print(\"drawing \"+output_file)\n",
    "    with open(output_folder+output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        block_cov.append(float(line.split(\" \")[1]))\n",
    "        line_cov.append(float(line.split(\" \")[2]))\n",
    "    line_cov_list.append(line_cov)\n",
    "    block_cov_list.append(block_cov)\n",
    "plt.figure(figsize=(6, 4))\n",
    "for line_cov, tool, color in zip(line_cov_list, tool_list, color_list):\n",
    "    draw_one_line(line_cov, tool, color)\n",
    "plt.legend(fontsize='small')\n",
    "plt.xticks(np.arange(0, 25, 1))\n",
    "plt.yticks(np.arange(0, 60, 5))\n",
    "plt.margins(x=0, y=0)\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 4))\n",
    "for block_cov, tool, color in zip(block_cov_list, tool_list, color_list):\n",
    "    draw_one_line(block_cov, tool, color)\n",
    "plt.legend(fontsize='small')\n",
    "plt.xticks(np.arange(0, 25, 1))\n",
    "plt.yticks(np.arange(0, 45, 5))\n",
    "plt.margins(x=0, y=0)\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "regex = r\"Can be detected by (.*)\\.\"\n",
    "lang_results, diff_results = {}, {}\n",
    "lang_dir = \"../data/experiment/anomalies-lang/\"\n",
    "for f in os.listdir(lang_dir):\n",
    "    with open(lang_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    # print(\"second_line:\"+second_line)\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in lang_results:\n",
    "        lang_results[tool] += 1\n",
    "    else:\n",
    "        lang_results[tool] = 1\n",
    "print(tabulate(lang_results.items(), headers=[\"Tool\", \"#Anomalies via language-level test\"]))\n",
    "print(\"\\n\")\n",
    "abl_dir = \"../data/experiment/anomalies-diff/\"\n",
    "for f in os.listdir(abl_dir):\n",
    "    with open(abl_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in diff_results:\n",
    "        diff_results[tool] += 1\n",
    "    else:\n",
    "        diff_results[tool] = 1\n",
    "print(tabulate(diff_results.items(), headers=[\"Tool\", \"#Anomalies via differential testing\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe these anomalous behaviors in detail, you can execute all test cases by running the following cell. This will take about 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "def run_one_testcase(cmd, dst_folder, testcase):\n",
    "    for root, dirs, files in os.walk(dst_folder):\n",
    "        for dirName in dirs:\n",
    "            if dirName == \"bin\" or dirName == \"obj\":\n",
    "                shutil.rmtree(os.path.join(dst_folder, dirName), ignore_errors=True)\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=dst_folder)\n",
    "    print(\"Output of \"+\"\".join(cmd)+f\" for {testcase}:\")\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "\n",
    "def run_all_testcases(src_folder, dst_folder, cmd_list):\n",
    "    for testcase in os.listdir(src_folder):\n",
    "        src_file = os.path.join(src_folder, testcase)\n",
    "        dst_file = os.path.join(dst_folder, \"Program.qs\")\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        for cmd in cmd_list:\n",
    "            run_one_testcase(cmd, dst_folder, testcase)\n",
    "\n",
    "proj_path = \"../data/experiment/temp-q-project\"\n",
    "\n",
    "lang_path = \"../data/experiment/anomalies-lang/\"\n",
    "print(\"==Start to run anomalies via language-level testing==\")\n",
    "run_all_testcases(lang_path, proj_path, [['dotnet', 'run']])\n",
    "\n",
    "diff_path = \"../data/experiment/anomalies-diff/\"\n",
    "print(\"==Start to run anomalies via language-level testing==\")\n",
    "run_all_testcases(diff_path, proj_path, \n",
    "                  [['dotnet', 'run'], \n",
    "                  ['dotnet', 'run', '-s', 'ToffoliSimulator'], \n",
    "                  ['dotnet', 'run', '-s', 'SparseSimulator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Results for RQ3 (Section 5.3)\n",
    "\n",
    "When conducting ablation study, we evaluate two variants of Upbeat: Upbeat-A and Upbeat-B. Upbeat-A removes the inputs generator and keeps other parts of Upbeat unchanged. Upbeat-B removes the code segment assembler and keeps other components.\n",
    "\n",
    "The Upbeat components all positively contribute to the bugexposing capability of the framework. Run the following two cells to observe the bugs discovered by Upbeat-A, Upbeat-B and Upbeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abl_results = {}\n",
    "abl_dir = \"../data/experiment/ablation-study/\"\n",
    "for f in os.listdir(abl_dir):\n",
    "    with open(abl_dir+f) as fi:\n",
    "        second_line = fi.readlines()[1]\n",
    "    match = re.search(regex, second_line)\n",
    "    tool = match.group(1)\n",
    "    if tool in abl_results:\n",
    "        abl_results[tool] += 1\n",
    "    else:\n",
    "        abl_results[tool] = 1\n",
    "print(tabulate(abl_results.items(), headers=[\"Tool\", \"#Bugs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Results for RQ4 (Section 5.4)\n",
    "\n",
    "We use two metrics, Recall and Precision, to evaluate the completeness and correctness of Upbeat-extracted constraints. \n",
    "\n",
    "The first computes the ratio of Upbeat-recognized constraints to the total number of constraints. The second metric computes the ratio correctly extracted constraints samples to the total number of constraints.\n",
    "\n",
    "Upbeat is capable of extracting the majority of constraints from both source code and API documents with high accuracy. Execute the following cell to view the detailed analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def get_rate(num1: int, num2: int):\n",
    "    if num2 == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return num1 / num2\n",
    "\n",
    "def convert_to_percent(n):\n",
    "    n = round(n, 2)\n",
    "    # print(\"n:\",n)\n",
    "    return \"%.0f%%\" % (n * 100)\n",
    "\n",
    "def calculate(d: dict):\n",
    "    classical_id, classical_ex, quantum_id, quantum_ex = 0.0, 0.0, 0.0, 0.0\n",
    "    classical_id_total, classical_ex_total, quantum_id_total, quantum_ex_total = 0, 0, 0, 0\n",
    "    for namespace, properties in d.items():\n",
    "        classical_id += get_rate(properties[\"classical-identified\"], properties[\"classical-id-total\"])\n",
    "        classical_ex += get_rate(properties[\"classical-extracted\"], properties[\"classical-ex-total\"])\n",
    "        quantum_id += get_rate(properties[\"quantum-identified\"], properties[\"quantum-id-total\"])        \n",
    "        quantum_ex += get_rate(properties[\"quantum-extracted\"], properties[\"quantum-ex-total\"])\n",
    "        if properties[\"classical-id-total\"] != 0:\n",
    "            classical_id_total += 1\n",
    "        if properties[\"classical-ex-total\"] != 0:\n",
    "            classical_ex_total += 1\n",
    "        if properties[\"quantum-id-total\"] != 0:\n",
    "            quantum_id_total += 1\n",
    "        if properties[\"quantum-ex-total\"] != 0:\n",
    "            quantum_ex_total += 1\n",
    "    # print(\"quantum_extracted:\", quantum_ex)\n",
    "    return convert_to_percent(classical_id / classical_id_total), convert_to_percent(classical_ex / classical_ex_total), \\\n",
    "           convert_to_percent(quantum_id / quantum_id_total), convert_to_percent(quantum_ex / quantum_ex_total)\n",
    "\n",
    "with open(\"../data/experiment/constraint-extraction/source-code.json\") as f1:\n",
    "    code_dict = json.load(f1)\n",
    "code_result = calculate(code_dict)\n",
    "tab = [(\"Source Code\", \"classical\", code_result[0], code_result[1]), (\"\", \"quantum\", code_result[2], code_result[3])]\n",
    "with open(\"../data/experiment/constraint-extraction/api-document.json\") as f2:\n",
    "    doc_dict = json.load(f2)\n",
    "doc_result = calculate(doc_dict)\n",
    "tab.append((\"API Document\", \"classical\", doc_result[0], doc_result[1]))\n",
    "tab.append((\"\", \"quantum\", doc_result[2], doc_result[3]))\n",
    "print(tabulate(tab, headers=[\"Source\", \"Type\", \"Recall\", \"Precision\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
